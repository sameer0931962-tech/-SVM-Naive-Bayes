{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1 :  What is Information Gain, and how is it used in Decision Trees?"
      ],
      "metadata": {
        "id": "bPz9mM-PBg5h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Information Gain (IG)\n",
        "\n",
        "Information Gain is a metric used in Decision Trees to decide which feature should be used to split the data at each node.\n",
        "It tells us how much uncertainty (impurity) is reduced after splitting the dataset on a particular feature."
      ],
      "metadata": {
        "id": "POWVDyRsBpTI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 2: What is the difference between Gini Impurity and Entropy?\n",
        "# Hint: Directly compares the two main impurity measures, highlighting strengths,\n",
        "# weaknesses, and appropriate use cases."
      ],
      "metadata": {
        "id": "fjnmi0g5BySJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gini Impurity** and **Entropy** are two commonly used impurity measures in decision tree algorithms to evaluate the quality of a split. Both aim to measure how mixed or impure a dataset is, but they differ in their interpretation and computation. Entropy measures the amount of uncertainty or randomness in the data and is calculated using logarithmic functions, making it slightly more complex and computationally expensive. It is more sensitive to changes in class probabilities and is often considered more theoretically informative. In contrast, Gini Impurity measures the probability that a randomly selected data point would be incorrectly classified if it were labeled according to the class distribution of the node. It uses squared probabilities, making it simpler and faster to compute. Because of this efficiency, Gini Impurity is widely used in practice, especially for large datasets, and is the default criterion in CART-based decision trees. While entropy can produce slightly purer splits in some cases, in most real-world scenarios both measures yield very similar results, with the choice often depending on performance needs and implementation preference.\n"
      ],
      "metadata": {
        "id": "95MbkA1cB8yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 3:What is Pre-Pruning in Decision Trees?"
      ],
      "metadata": {
        "id": "D33qZDhZCOaV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-Pruning in Decision Trees\n",
        "\n",
        "Pre-Pruning (also called early stopping) is a technique used in Decision Trees to stop the growth of the tree before it becomes too complex. Instead of allowing the tree to grow fully and then trimming it later, pre-pruning halts further splitting of a node when certain conditions are met, such as when the information gain is too small, the node contains very few samples, or a maximum tree depth has been reached."
      ],
      "metadata": {
        "id": "mZC9dK6CCT6s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 4:Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances (practical).\n",
        "## Hint: Use criterion='gini' in DecisionTreeClassifier and access .feature_importances_.\n",
        "## (Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "uc-0tL4qCXL1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is a practical Python example that trains a Decision Tree Classifier using Gini Impurity and prints the feature importances, exactly as asked."
      ],
      "metadata": {
        "id": "HHhEb1tMCj42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Train Decision Tree with Gini Impurity\n",
        "model = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "model.fit(X, y)\n",
        "\n",
        "# Create a DataFrame for feature importances\n",
        "feature_importances = pd.DataFrame({\n",
        "    'Feature': iris.feature_names,\n",
        "    'Importance': model.feature_importances_\n",
        "})\n",
        "\n",
        "# Print feature importances\n",
        "print(feature_importances)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8C5hHno2Coj4",
        "outputId": "bf5c635b-7b2e-4942-8df5-775b101f4189"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             Feature  Importance\n",
            "0  sepal length (cm)    0.013333\n",
            "1   sepal width (cm)    0.000000\n",
            "2  petal length (cm)    0.564056\n",
            "3   petal width (cm)    0.422611\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 5: What is a Support Vector Machine (SVM)?"
      ],
      "metadata": {
        "id": "N2w2ZerDDBBB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Support Vector Machine (SVM)**\n",
        "\n",
        "A **Support Vector Machine (SVM)** is a **supervised machine learning algorithm** used for **classification and regression** tasks. Its main objective is to find an **optimal decision boundary (called a hyperplane)** that separates data points of different classes with the **maximum possible margin**. The data points that lie closest to this boundary are known as **support vectors**, and they play a critical role in defining the position of the hyperplane."
      ],
      "metadata": {
        "id": "1yBord6hDDwa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 6:  What is the Kernel Trick in SVM?"
      ],
      "metadata": {
        "id": "k1P5oTWPDJOM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kernel Trick in Support Vector Machine (SVM)\n",
        "\n",
        "The Kernel Trick is a powerful technique used in Support Vector Machines (SVMs) to handle non-linearly separable data. When data cannot be separated by a straight line (or hyperplane) in its original feature space, the kernel trick allows SVM to implicitly map the data into a higher-dimensional space where a linear separation becomes possible—without explicitly computing that transformation. This makes the method both efficient and effective."
      ],
      "metadata": {
        "id": "EjdoBNvmBvqB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 7:  Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies.\n",
        "## Hint:Use SVC(kernel='linear') and SVC(kernel='rbf'), then compare accuracy scores after fitting on the same dataset.\n",
        "## (Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "iMLuqa71DtPe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Below is a complete practical Python program that trains two SVM classifiers (Linear and RBF kernels) on the Wine dataset and then compares their accuracies, exactly as asked."
      ],
      "metadata": {
        "id": "t7VS6H3RD59Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train SVM with Linear kernel\n",
        "svm_linear = SVC(kernel='linear')\n",
        "svm_linear.fit(X_train, y_train)\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "linear_accuracy = accuracy_score(y_test, y_pred_linear)\n",
        "\n",
        "# Train SVM with RBF kernel\n",
        "svm_rbf = SVC(kernel='rbf')\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "rbf_accuracy = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "# Print accuracies\n",
        "print(\"Linear Kernel Accuracy:\", linear_accuracy)\n",
        "print(\"RBF Kernel Accuracy:\", rbf_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TtOurL4D-Ao",
        "outputId": "cc115341-bbe0-4eb8-cd37-01c64b1d88ac"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Kernel Accuracy: 0.9814814814814815\n",
            "RBF Kernel Accuracy: 0.7592592592592593\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 8: What is the Naïve Bayes classifier, and why is it called \"Naïve\"?"
      ],
      "metadata": {
        "id": "AbZlgzYiEGfZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Naïve Bayes Classifier**\n",
        "\n",
        "The **Naïve Bayes classifier** is a **supervised probabilistic machine learning algorithm** based on **Bayes’ Theorem**, which is used mainly for **classification tasks** such as spam detection, sentiment analysis, and document classification. It predicts the class of a data point by calculating the **posterior probability** of each class given the input features and then choosing the class with the highest probability.\n",
        "\n",
        "It is called **“Naïve”** because it makes a **strong simplifying assumption** that **all features are conditionally independent of each other given the class label**. In real-world data, this assumption is often not true—for example, in text classification, words are usually related—but surprisingly, Naïve Bayes still performs very well in many practical applications.\n",
        "\n",
        "Despite its simplicity, Naïve Bayes is **fast, scalable, and effective with high-dimensional data**, especially in text-based problems. However, its performance can degrade when features are highly correlated.\n"
      ],
      "metadata": {
        "id": "FJijbDs2EJTZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 9: Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes"
      ],
      "metadata": {
        "id": "V1vhDpPMERmq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Differences Between Gaussian, Multinomial, and Bernoulli Naïve Bayes**\n",
        "\n",
        "Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes are three variants of the Naïve Bayes classifier that differ mainly in the **type of data they are designed to handle** and the **probability distribution they assume for features**. **Gaussian Naïve Bayes** assumes that continuous features follow a normal (Gaussian) distribution, making it suitable for real-valued data such as height, weight, or sensor readings. **Multinomial Naïve Bayes** is designed for discrete count-based data and is widely used in text classification problems like spam detection, where features represent word frequencies or term counts. **Bernoulli Naïve Bayes**, on the other hand, works with binary features and models whether a feature is present or absent, which is especially useful for text classification using binary word occurrence rather than counts. In summary, Gaussian Naïve Bayes is best for continuous data, Multinomial Naïve Bayes is ideal for frequency-based discrete data, and Bernoulli Naïve Bayes is suitable when features are binary; the choice depends on the nature of the dataset and feature representation.\n"
      ],
      "metadata": {
        "id": "PIIoBYI5EaYM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 10:  Breast Cancer Dataset Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer dataset and evaluate accuracy.\n",
        "### Hint:Use GaussianNB() from sklearn.naive_bayes and the Breast Cancer dataset from sklearn.datasets.\n",
        "### (Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "7TCEmunGEhe1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Below is a complete practical Python program that trains a Gaussian Naïve Bayes classifier on the Breast Cancer dataset and evaluates its accuracy, exactly as asked."
      ],
      "metadata": {
        "id": "tk3p783EE3qm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train Gaussian Naïve Bayes classifier\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print accuracy\n",
        "print(\"Gaussian Naïve Bayes Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOA0hqHmE2_O",
        "outputId": "07eae9d7-b414-4e1e-e9bd-0b7ab14b5075"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gaussian Naïve Bayes Accuracy: 0.9415204678362573\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "k3yRT1JoE9pP"
      }
    }
  ]
}